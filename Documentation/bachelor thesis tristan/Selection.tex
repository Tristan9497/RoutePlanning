\chapter{Implementation}
\label{Selection}

This chapter covers the package selection and the setup of all nodes in the concept. In addition to that the development of the custom nodes will be discussed. This results in nodes assigned to each block in the schematic pictured in Figure \ref{navconcept}.

These nodes are then configured in order to fulfill the requirements.

\section{Simulation}
There are many options, when it comes to robot simulation environments, which makes a proper selection mandatory. The chosen simulator then needs to be configured and equipped with models, sensors and a drive system.

\subsection{Selection}
To begin of the selection process a group of reasonable simulators needs to be collected. The two selected options are Gazebo and V-REP since they are the two most used robotics 3D simulators \cite{SimComp}.\\
The newest version of V-Rep is now called CoppeliaSim. Unfortunately, there are no comparisons between it and Gazebo yet. Performing a thorough comparison between both would exceed the scope of this thesis. Therefore, its predecessor V-Rep will be compared. \\

Both simulators seem to fulfill most of the defined requirements to a certain extend, while Gazebo seems to have an easier installation process and integration into ROS.\\
Gazebo is included in the default packages of ROS Noetic since it is developed by the Open Source Robotics Foundation as the default simulator for ROS\cite{ROSPkg}.

V-REP, as well as Gazebo, offers plugins and URDF conversion for custom models, but an even bigger selection of mobile robot models. Unfortunately, they can only be used as examples since the required models are very specific.\\
In contrast to V-REP, Gazebo does not contain an integrated model editor.

The comparison in the paper of L. Pitonakova et al. regarding computational load shows, that V-Rep is the most resource hungry environment, when compared to  Gazebo\cite{Pitonakova}. Real time simulation is highly wanted in this thesis, so the results can be compared to real results in the future.

Based on the fact, that Gazebo is included in the ROS-Noetic package list and has a smaller computational load compared to V-Rep, it will be the simulation environment in this project.

\subsection{Model}
Since Gazebo doesn't have an integrated model editor the freeware ``Blender'' will be used for the generation of the model of the environment, since it has options for exporting COLLADA files (.dae) usable in Gazebo.

\subsection{Configuration}
The following sections cover the configuration and the generation of the models used in the simulation, together with all simulated sensors and the motor controller.


\subsubsection{URDF and robot\_state\_publisher}

The URDF model is based on the model of Christen Lofland, who created this for his own open source project\cite{chrisl8}.\\

Unfortunately, this model has been created to generate the fixed transforms of a real model only, so it does not have moveable joints for the wheels or any plugins for sensors and the differential steering. In addition to that it uses .stl files for both visual and collision volumes. ``.stl'' files consist of connected triangles, that approximate the shape. The approximation has always a slight error to the original shape, creating unpredictable collision points.
To keep the model as simple as possible one common Xacro file is used, that loads the individual files for the following parts:
\begin{itemize}
	\item Arlo body
	\item Camera
	\item Lidar
	\item IMU
	\item Gazebo plugins for all sensors and differential drive
\end{itemize}

All of the files are Xacro files in contrast to pure URDF so parameters and macros can be used, which makes the robot model more flexible. This allows to introduce parameters, for example for the position of the sensor, which makes the files easier for future modifications.\\

The plugins for Gazebo have not been included in the individual files so the robot description can be used for real robots with a similar sensor setup without big modification but just with the exclusion of the plugin file.

The resulting robot then looks like pictured in Figure \ref{arlourdf}.

\begin{figure} 
	\includegraphics[width=\textwidth]{Pictures/arlourdf}
	\caption{Arlo URDF with sensors and Gazebo plugins}
	\label{arlourdf}
\end{figure}


Using rqt\_tf\_tree the tf tree can be checked for conformity to ``REP 105'' and the concept.\\ 

A simplified version of the tf\_tree is picture in \ref{tfsimp}, whereas the complete one is picture in Appendix \ref{config}.\\

\begin{figure} 
	\centering
	\includegraphics[width=.5\textwidth]{Pictures/tf tree simplified}
	\caption{Simplified tf\_tree}
	\label{tfsimp}
\end{figure}

As visible the fixed frame of the setup is ``map'' the following transform to the ``odom'' frame is published by Cartographer. The transformation between the ``odom'' frame and ``base\_footprint'' then gets published by robot\_localization.\\
The remaining transformations and frames are published by ``robot\_state\_publisher'' according to the URDF of the robot.

\subsubsection{Gazebo}
The setup of the simulation consists out of the modeling of the world, the conversion of the model into a sdf file and the general launch file.

As described in the concept the world will be modeled using the software Blender.\\

The modeling itself is very straight forward since one segment of the road can be modeled and extruded along a closed path like pictured in figure \ref{simworld}. 

The course of this road has been constructed to feature the following elements:

\begin{itemize}
	\item Left and right curves
	\item Tight and wide curves
	\item Long straight section
\end{itemize}

It will be the environment for most of the following tests.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{Pictures/test track}
	\caption{Finished simulation world used for various tests}
	\label{simworld}
\end{figure}

Gazebo then can be started from a launch file by defining the generated world.\\

\subsubsection{Plugins}
The configuration of the sensor and drive plugins are fairly straight forward, based on the tutorial page in the Gazebo documentation\cite{gazebotutorial}.\\



While trying to simulate the original camera of the robot based on its datasheets (Appendix \ref{AppendixDataSheets}) and the calibration parameters, it is observable that not all parameters of the camera\_info message can be set in the sdf description of the plugin.

Overall it does not seem to be possible to mimic the original camera, hence the camera will be modeled without distortion with the assumption that a calibrated camera and image rectification packages like ``image\_proc'' would produce a comparable result.








\section{move\_base}

As described in the concept, the navigation stack will be used in this thesis. Therefore, the action move\_base will be used to get the robot to a given goal.\\
 move\_base incorporates nav\_core and costmap\_2d and their internal nodes, which need to be selected according to the requirements of the navigation.\\
Similar to the separation described in the concept (chapter \ref{Concept})the nodes will be categorized into a global and local stage and discussed individually.

\subsection{Global stage}
\label{globalconfig}
\subsubsection{Planner}

Since the global planner needs to communicate with the nav\_core interface  the following selection as described in the documentation of nav\_core can be used as possible candidates\cite{navcore}:

\begin{itemize}
	\item global\_planner
	\item navfn
	\item carrot\_planner
\end{itemize}


Here the choice is fairly easy, since base\_global\_planner is the successor of navfn. It still supports the the behavior of navfn, but it offers more options, such as A* planning algorithm instead of Dijkstra.


carrot\_planner isn't suited for this use-case since it doesn't fullfil the requirement of being able to cover lane changes since it will not generate plans, that go around obstacles. Instead it will generate a straight path to the goal and shortens the path if the goal is behind or in an obstacle\cite{corrotplanner}.




\subsubsection{Costmap}
Since costmap\_2d is embedded in move\_base the package does not need to be selected itself. Instead the layers of the individual costmap need to be selected.\\

As defined in the concept, the global costmap needs to have information about lethal obstacles, road markings, as well as a defined preference for the right lane.\\


The data, supplied by the lidar and the road detection, can be marked as lethal obstacles in the costmap using the obstacle layer. This data consist solely from points, meaning there are gaps between the individual measurements. In the case of the lidar, the size of these gaps increases with the measurement distance.\\

As soon as these gaps are larger than the resolution of the costmap, free cells will start to appear between the measurements, that allow the planner to generate paths through obstacles or the road markings.\\

To solve this issue the default layer ``inflation\_layer'' can be used. This layer inflates every lethal cell in the costmap by a fixed radius, closing the previously discussed gaps like pictured in figure \ref{costinfl}.\\

\begin{figure} 
	\centering
	\includegraphics[width=.7\textwidth]{Pictures/costmap inflation}
	\caption{Visualization of the inflated sensor data in the costmap (red - road detection, green - lidar scan, blue - inflation)}
	\label{costinfl}
\end{figure}

This configuration allows the navigation to plan around obstacles, without leaving the road, but there is no preference for the right lane yet. To generate this preference the left lane has to have higher cost than the right lane, without being lethal.

Unfortunately, this can not be solved using the provided layers. Therefore a custom layer has to be developed, that will be named dynamic\_cost\_layer.

The only information about where the lane is, comes from the road detection, which outputs polynomials that approximate the road markings. The aim is to make a gradually increasing cost from the middle of the right lane to the left road marking. Therefore, the left road marking needs to be inflated using dynamically adjustable parameters for the cost distribution.\\


To guarantee, that the robot is driving on the right lane, the right road marking will be inflated, but with a different cost distribution. Here only the road marking is lethal, whereas the lane should be free resulting in the cross section pictured in \ref{globalcostdistro}. To simplify lane changes in the case of obstacle avoidance, these obstacles need to have a cost free zone around them.\\

The layer should also feature a reset option as a recovery behavior.

\begin{figure} 
	\centering
	\includegraphics[width=.8\textwidth]{Pictures/global stage cost distro}
	\caption{Global costmap cross section of the road}
	\label{globalcostdistro}
\end{figure}



The previously discussed tasks for the layer can be summarized in the following requirements:

\begin{itemize}
	\item Input for points
	\item Input of point individual inflation parameters
	\item Rasterization of the individual cost distribution
	\item Service to reset the cost in the layer
\end{itemize}



\subsection{Local stage}
\subsubsection{Planner}
In contrast to the global planner there are more options for the local planner node to choose from. Like the global planners the local planners will be selected using the options offered in the description of nav\_core\cite{navcore}.

\begin{itemize}
	\item base\_local\_planner
	\item dwa\_local\_planner
	\item eband\_local\_planner
	\item teb\_local\_planner
	\item mpc\_local\_planner
\end{itemize}

To choose the local planner the requirements have to be defined first.\\

Since the global planner is taking care of the obstacles and the road lanes, the local planner has the general task of following the global path and creating a command velocity that is feasible in regards to the dynamics of the robot.\\

Smooth lane changes are highly wanted in this project. This will help the camera and therefore the road detection to keep seeing the road during a lane swap. To achieve this, the planner is supposed to drive close to the global plan and to smooth out the edges.
Furthermore the local planner needs to have a good performance in tight corridor situations, since those will often be caused by obstacles blocking one lane.\\

During all of this the local planner needs to use the information of lethal obstacles to prevent crashes in its optimized path.\\


Base and DWA are two planners that are included in the navigation stack. Similar to the global planner selection, the successor and therefore DWA seems like a good option. According to Kaiyu Zeng, this is the general recommendation for mobile robotics platforms\cite{navtuningguide}. As described in the theoretical background, DWA struggles with navigation in narrow corridors and therefore can not be used.

In contrast to the method of the dwa planner, the elastic band approach is better suited for narrow corridors. The two planner, that use this approach are eband\_local\_planner and teb\_local\_planner. Choosing between these two is difficult, since they both share the same base principle. teb\_local\_planner is better suited for this use case, since it supports both car like and differential robots.

mpc\_local\_planner is build using the popular model predictive control approach. According to Christoph Rösmann (the developer of both planners), teb is preferred when dealing with simple differential or car like robots\cite{mpcvsteb}. 

According to the previous analysis teb\_local\_planner has the best fit to the use case and is therefore selected for the navigation.


\subsubsection{Costmap}
The local planner is only supposed to generate velocity commands that lead to no lethal collision. Therefore, the local costmap will have the same layers as the global one without the dynamic\_cost\_layer that is used to guide the robot to the right lane.

\subsection{Configuration}
\subsubsection{base\_global\_planner}
\label{globalplannertest}
There is not much room for configuration, when it comes to the global\_planner of the navigation\_stack. Probably the most important step for computation load is the choice of a planning algorithm.\\
The two algorithms that are offered by the global\_planner are Dijkstra and A*.\\

\begin{figure} 
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\textwidth]{Pictures/Dijkstra}
		\caption{Dijkstra}
	\end{subfigure}	
	%\hskip2em
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\textwidth]{Pictures/AStar2}
		\caption{A*}
	\end{subfigure}

	\caption{Planning algorithm comparison (grey cells are not observed)\cite{globalplanner}}
	\label{plannercomparison}

\end{figure}


When looking at the observed cells, it is obvious, that A* is much more efficient in this use case, since the robot will go straight or in a slight curve. Therefore, A* will be chosen for the global planner.\\

Another important setting is necessary since the global costmap is used as a rolling window costmap. The global\_planner will by default outline the global costmap with lethal cells, to prevent the global planner to plan outside of a fixed costmap. This behaviour results in artifacts, after the map has moved together with the robot which hinder the planners from finding a path as pictured in \ref{boardererror}.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Pictures/borders}
	
	\caption{Global planner border error}
	\label{boardererror}
\end{figure}

To prevent this behavior the not documented parameter ``outline\_map'' has to be set to false. The default value of this parameter is therefore changed in the forked version of the navigation stack.

To make planning easier the parameter ``cost\_factor'' can be reduced. This parameter multiplies the cost of every cell in the costmap before planning, which
 would make the gradually decreasing cost of the dynamic\_cost\_layer redundant, since the value of the cell would be multiplied and the resulting cost would be considered as lethal.
 
 
 While using the global\_planner plugin a bug during the potential field calculation has been observed as pictured in \ref{potentialfield}. This bug causes an error of the planner, since it is not able to resolve a path, even though the potential field exists. Which therefore leads to the robot continuing to drive with the same velocities,  until the planner manages to construct a valid potential field again. In this moment the robot often leaves the right lane even though there is no obstacle in front of it.\\
 
 This bug has been reported to the developer of the plugin. 
 \begin{figure} [H]
 	\centering
 	\includegraphics[width=.7\textwidth]{Pictures/out of bounds}	
 	
 	\caption{Potential field generated by global\_planner plugin}
 	\label{potentialfield}
 \end{figure}
 
 \subsubsection{teb\_local\_planner}
Like the global planner the local planner has to be configured to fulfill its tasks.

A good starting point for the configuration are the example configurations in the repository of the developer of the planner\cite{tebtutorials}. They offer a base configuration for both differential and car like drive, as well as omnidirectional robots.\\

Unfortunately, teb\_local\_planner only considers lethal cost and without any configuration would follow the global path very loosely resulting in e.g. cutting corners.\\ 

To give the planner a tendency to follow the global planner closer the option ``viapoint'' can be used. This allows to set points at a configurable distance to each other on the global path, that pull the local path and the robot to the correct lane. The attraction of the local path through the via points is then tuned so the robot drives roughly in the middle of the right lane, when no obstacle is on the road, but still can separate itself from the global path, when avoiding obstacles.\\

The parameter ``max\_global\_plan\_lookahead\_dist'' controls how much of the global plan is actually considered by the local planner. This parameter highly influences the computational load when setting the distance to larger values. Using shorter distances results in oscillations while driving straight, which is caused by the jumping global path.\\

During testing teb\_local\_planner produced good and feasible paths, but sometimes it generated loops along the global path, that would result in the robot turning in spot, before it follows the global path further. This leads to the robot changing the driving direction since after half of a turn the PoseFinder finds new goals, that are feasible.\\

To suppress this behavior the weighting of the time component has been increased so the internal forces that contract the elastic band increase. This reduced the occurrence of this scenario significantly, but the local path takes longer, until it is on the right lane again, since the distance to the global path is now weighted less relative to the weight of the time component.

\subsubsection{Costmaps}

As mentioned, both costmaps share the same layers, but the global costmap has the dynamic\_cost\_layer in addition. Therefore, the configuration is large the same and will be covered only once.

The costmaps will both have the same size that should always be larger than the distance, at which the PoseFinder searches for goals.

Since both costmaps are rolling window costmaps they will reference the continuous frame ``odom'' and move with the frame ``base\_footprint''.

The obstacles seem to move slightly based on sensor noise. To prevent the representation of these obstacles to collide with the robot, the robot radius is set smaller than the robot actually is. Otherwise these obstacles can collide with the footprint of the robot and the global planner can not produce a valid path that the local planner can follow.\\

This footprint is only considered by the global planner, which is only required to provide a rough path. The local planner has its own setting for a footprint, therefore the obstacle avoidance will still work as expected.

The last remaining settings are the resolutions and the frequencies of the costmaps, which are chosen based on the performance of the navigation and the computational load.

 
\section{PoseFinder}
As described in the concept the purpose of this node is to determine the pose of the next goal and send it to the move\_base action client. Therefore, it will need to process the data from the road detection and if available from the SLAM map to determine a feasible goal.\\

Since the robot should never reach a goal, the PoseFinder needs to determine new goals at a configurable frequency. 

\subsection{Usage of Current Camera Data}


The easiest way to get new goals, is to take the last point of the polynomial provided by the road detection as the position and calculate the yaw angle of the new goal with the gradient of the last two points.\\

While this is a logical approach in an ideal scenario, it certainly will not work in a realistic one, since a continuous data stream from the road detection can not be guaranteed, as shown in the edge cases pictured in figure \ref{nav edge case}.\\

In all examples pictured in figure \ref{nav edge case}, the road detection is not able to detect the road. If the robot would only follow the lanes detected by the road detection, it would not be able navigate in these situations, leading to stop of the robot.\\

This introduces the requirement for the PoseFinder, to find goals by estimating the course of the road, based on the latest data of the road detection. Therefore the navigation bridges the gap, in which the camera can not see the road.\\

\begin{figure}
	\centering
	\begin{subfigure}{.24\linewidth}
		\includegraphics[width=\textwidth]{Pictures/road detection obstacle}
		
		\caption{Lane markings blocked by obstacle}
	\end{subfigure}
	%\hskip2em
	\begin{subfigure}{.24\linewidth}
		\includegraphics[width=\textwidth]{Pictures/road detection angle}
		\caption{Driving in a corner}
	\end{subfigure}	
	%\hskip2em
	\begin{subfigure}{.24\linewidth}
		\includegraphics[width=\textwidth]{Pictures/road detection blind}
		\caption{Camera angle during avoidance}
	\end{subfigure}
		%\hskip2em
	\begin{subfigure}{.24\linewidth}
		\includegraphics[width=\textwidth]{Pictures/road detection noise}
		
		\caption{Camera noise or bad road markings}
	\end{subfigure}


	\caption{Edge cases that prevent simple lane following}
	\label{nav edge case}
\end{figure}



\subsection{Approximations}
The approximation provided by the road detection is in the form of a polynomial. These describe the real road very accurately, but only in a very restricted domain. Outside of this domain, these polynomials diverge rapidly from the real road, making them unusable for any estmates.

Since the road mostly consists of circles of varying radii and origins, it is self evident, that using the polynomials in their restricted domain to represent a section of a circle will give a better estimate.\\
This circle can be calculated using the least square approach presented in Randy Bullocks paper on least square circle fitting \cite{leastsquarecircle}.\\

In theory this approximation should work for nearly straight sections as well. This will cause radius and origin to trend to infinity. Caused by camera noise and the inaccuracy of the road detection the representation of straight sections will get increasingly inaccurate.\\

Similar to the least square approach for circles, an approach for the lines can be formulated.

Switching between the result of the two approximations will produce the best result for both scenarios. This switching will be triggered by the radii of the approximated circles exceeding a configurable threshold.\\

The next step is a goal extraction from the chosen approximation. This goal will be calculated at a given distance from the robot origin on the approximated route.
 
\begin{figure} 
	\centering
	\includegraphics[width=\textwidth]{Pictures/goal finding}
	\caption{Goal extraction from circle approximation}
	\label{goalfinding}
\end{figure}

Figure \ref{goalfinding} shows the necessity to limit the angle and the distance, at which goals are extracted from the circles. With small circles, a fixed distance would result in not feasible goals. These would not be on the road and in addition to that would point in the wrong direction.

With large circles a fixed angle would cause very large distances. With increasing circle radii these get more and more unlikely to be on the road.\\

Therefore the goal on large circles needs to be extracted at a given arc length, whereas the goal on small circles needs to be extracted at a fixed angle. This angle is called ``angle of trustwothiness'', since it represents the amount of the circle that is likely to be a good representation of the road. The algorithm first checks the angle, produced by a given arc length. If that exceeds the angle of trustworthiness the distance is shrunk accordingly.\\

When extracting a goal from the lines only the distance limitation applies. The orientation of the goal will then be determined using the derivative of either, the approximated circles or lines.\\

With the calculated points on both circles or on both lines the mean can be calculated and represents the new goal for the robot.

\begin{figure} 
	\centering
	\begin{subfigure}{.45\linewidth}
		\includegraphics[width=\textwidth]{Pictures/circle approx}
		\caption{Circle approximation}
		\end{subfigure}	
	%\hskip2em
	\begin{subfigure}{.45\linewidth}
		\includegraphics[width=\textwidth]{Pictures/lineapprox}
		\caption{Line approximation}
	\end{subfigure}

	\caption{Approximations visualizations}
	\label{aproxvis}

\end{figure}

A visualization of the result is pictured in figure \ref{aproxvis}.
It shows the robot, its current goal, as well as the estimated road outlined by the geometric shapes of the respective approximation.


While these approximations allow the robot to bridge the edge cases pictured in \ref{nav edge case}, this approach introduces new problems. Since the goal is now determined by using estimations, it is uncertain if the goal is located off the road, or even in an obstacle. This problem is pictured in figure \ref{goaluncertainty}. Here the goal of the navigation is located within an obstacle. This would make path finding to the goal impossible.\\

To solve this issue the navigation has to consider obstacles, only if they are significantly closer to the robot than the goal. Therefore, the navigation treats the road as ``free'' until the obstacle is close to the robot. At this moment the predicted goal is way behind the obstacle, leading to an avoidance as described in the requirements of the navigation.

This does not only affect the distance, at which the PoseFinder estimates goals, but also the distance at which obstacles are treated as such and marked in the costmap, which has to be considered during the configuration.

\begin{figure}[H]
\centering
	\includegraphics[width=0.33\textwidth]{Pictures/finding goal in obstacle}
	\caption{Uncertainty of predicted goals}
	\label{goaluncertainty}
\end{figure}


\subsection{Goal from map}

If the robot is running a SLAM algorithm during the first round, the goal can be extracted directly from the SLAM map afterwards. This is more efficient and provides goals that will always be on the road. Furthermore the distance, at which goals will be found can be increased, this results in higher speeds of the robot and/or lower planner frequencies.\\

To find a goal in the SLAM map a circle rasterization algorithm will be used, based on Bresenham rasterization \cite{ComputerGraphics}.\\

This algorithm finds every cell on a circular path around the robot and its associated value in the map. The values outside of a given FOV (field of view) can be eliminated. The remaining values with a larger likelihood than a configurable threshold will be reduced to one point by taking the mean value of them.\\

The orientation of the goal is determined by using the previously explained approximation algorithms.

\subsection{Concept}

Based on the previously proposed ways to determine goals, the concept pictured in figure \ref{posefinder structure} can be deduced.\\

\begin{figure} 
	\centering
	\includegraphics[width=.9\textwidth]{Pictures/posefinder diagram}
	\caption{PoseFinder internal structure}
	\label{posefinder structure}
\end{figure}

The goalfinding algorithms are ordered based on their expected accuracy.

An accurate map of the surrounding offers the best potential to find feasible goals on the upcoming road. Hence, it has to be checked first, if a goal can or should be extracted from the map.

As described, every section can be expressed by a circle segment. This approximation will have a higher priority than the line approximation.

Finally the line approximation will be checked and only, if the circle approximation generated circles with radii, that exceed a threshold for their trustworthiness.

The result of the chosen algorithm can be used to determine a goal that will be sent as an input to move\_base.

The goal found by the approximation, unfortunately is not certainly on the road. It could as well be in an obstacle or of the road, which would make complete navigation to the goal impossible. Hence this procedure will be repeated at a configurable frequency, so the robot will never reach one of the goals. 

The node is configurable using rqt\_reconfigure and the following parameters.

\begin{table} 
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ l l l }
	
 \textbf{Parameter} & \textbf{Description} & \textbf{Range}\\
 cRadThresh & The Threshold for the radius of the circle approximation & 0.5-20 [m]\\ 
 goalDist & The maximum distance at which a goal should be found & 0.5-20 [m]\\  
 goalAngle & The angle of trustworthiness for the circle approximation higher priority than distance & 0.1-2 [$\pi$]\\  
 reductionRadius & Max distance at which points from the map will still be combined & 0.25-2.5 [m]\\  
 mapCostThresh & Threshold Cost Value for Map Data Extraction & 0-100\\  

\end{tabular}
}

\caption{PoseFinder parameters}
\label{posefinderparams}

\end{table}

\subsection{Configuration}


Starting with the goal extraction from the map, the parameter ``MapCostThresh'' will be set to 50, meaning the probability of being an obstacle has to be at least 50\%. The reduction radius has to be configured slightly larger than the road width, so the points extracted on both road markings can be combined, even if the road is slightly angled. With a road width of 1.8 meter, 2 meter will be chosen for this parameter.\\

The parameter ``GoalDist'' has to be configured smaller, than half of the costmap width so it can be guaranteed, that the goal is allways located inside the bounds of the costmap to prevent errors. With a costmap width and hight of 9 meter the parameter is set to 4 meter.\\

To switch between the circle and line approximation the parameter ``CRadThresh'' is set to 6m. Setting this parameter higher will improve the performance in wide turns while the performance on straight sections will get worse and vice versa.\\

Finally the GoalAngle for the circle approximation has to be configured. This is set to $0.3\pi$, meaning the window of trustworthiness of the circles is 60°.



\section{dynamic\_cost\_layer}
As described in the section \ref{globalconfig}, the global costmap requires a layer, that marks the left lane with increasing cost, to hinder the robot from leaving the right lane. This section builds upon the requirement definition in chapter \ref{Concept}.\\


To keep this layer flexible for other use cases, the generalized task of custom point inflation can be formulated. At first this seems similar to the inflation layer of the navigation stack, but the inflation layer inflates every lethal cell by one fixed distribution, whereas this layer is supposed to handle a custom distribution for every cell.\\

This behavior can be used to inflate the left road marking in the global costmap to force the global plan on the right side of the road, which is the behavior defined in the concept. The pluginis also supposed to be used to inflate cells with zero cost, which can be useful to guarantee a cost free right lane, or to give some free space around obstacles located on the road.\\


Figure \ref{dynnav} shows the cost zones generated by the dynamic\_cost\_layer, and how the path finding algorithms could profit from this. The red sections are the ones with the actual cost distribution, whereas the blue and green sections are the zones, that are removed by the right road marking and the obstacle.\\

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{Pictures/inflationlayernav}
	\caption{Navigation on road with dynamic\_cost\_layer}
	\label{dynnav}
\end{figure}


The layer receives a point cloud on a configurable topic. This point cloud is expected to feature channel values for the inflation radius, the maximum and the minimum cost for each individual point.\\

Since it can not be assumed, that the incoming points will be in the frame of the costmap the points in the costmap have to be transformed into the right frame using tf2.\\

Assuming a small resolution of the costmap and large inflation radii, the inflation algorithm has to compute and set a substantial amount of costs for each point of the point cloud. Therefore, efficiency is very important to being able to handle large point clouds in real time.\\

To minimize the computation load a Bresenham based algorithm for the circle rasterization will be used\cite{ComputerGraphics}. The point symmetry around the cell can be used to further minimize the computational load and only $\frac{1}{8}$th of the circle has to be computed. The rasterization process can be described by black cells on the perimeter of the circle in figure \ref{rasterization}.\\

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{Pictures/rasterization}
	\caption{Modified Bresenham rasterization with efficient surface filling}
	\label{rasterization}
\end{figure}


Adding to the typical behavior of the Bresenham rasterization the area of the circle will be filled using the point symmetry and by skipping overlapping points of the lines like in Figure \ref{rasterization}. Here it is visible, that every row with the same color is only calculated once and then projected in all eight octants. 
The cells within the circles perimeter are filled with the cost specified for that point. For this the following linear decaying \nth{1} degree function will be used which requires the computation of the distance of the rasterized cell to the center of the circle.

\[cost(distance)=maxcost-distance*\frac{maxcost-mincost}{radius}\]\\
with: \[distance=\sqrt{cell.x^2+cell.y^2}\]

 Since this will still require the usage of a square root for each cell in the circle This will be optimized as well.\\

The goal here is to use a function that contains only the squared distance and not many other mathematical operators, which still represents a decaying trend. This requirement rules out every function with an odd degree, all functions with an x offset and the normal distribution. This leaves all functions with an even degree from which the \nth{2} degree function is chosen to reduce square operators. The comparison between the two functions can be seen in figure \ref{distrcomp}.

\[cost(distance)=maxcost-distance^2*\frac{maxcost-mincost}{radius^2}\]\\

\begin{figure} 
	\begin{center}
	\includegraphics[width=140mm]{Pictures/linear cost comparison}
	\caption{Cost distribution comparison with maxcost=250 mincost=100 radius=1}
	\end{center}
	\label{distrcomp}
\end{figure}

When adding the inflation around points, the maximum of the calculated cost value and the current value of the cell in the costmap is set.
This leads to a problem, when trying to clear zones in the costmap, since then the minimum cost of the two needs to be set. Therefore setting zero as the cost value has the privileges for overwriting high cost values in the costmap.
 To prevent the cleared cells from being overwritten by the inflation of the next point, the clearing points have to be the last points in the input point cloud.\\

Figure \ref{dyncost} shows an avoidance of a simulated obstacle and the cost distribution in the costmap, generated by this layer. It is noticeable, that the right lane (in driving direction) has a clear zone over the entire length. Furthermore the cost around the points detected by the lidar have been removed.


\begin{figure} 
	\centering
	\begin{subfigure}{.45\linewidth}
		\includegraphics[width=\textwidth]{Pictures/avoidance}
		\caption{Simulation environment}
		\end{subfigure}	
	%\hskip2em
	\begin{subfigure}{.45\linewidth}
		\includegraphics[width=\textwidth]{Pictures/avoid cost}
		\caption{Costmap filled by the dynamic\_cost\_layer}
	\end{subfigure}
	\caption{Resulting cost during avoidance}
	\label{dyncost}
\end{figure}


The layer can be configured individually for the costmap using rqt\_reconfigure and the following parameters.




\begin{table} 
\centering
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{ l l l }
	
 	\textbf{Parameter} & \textbf{Description} & \textbf{Range}\\
 	enabled & Whether to apply this plugin or not & [bool]\\
 	point\_topic & The Topic of the PointCloud containing Points and channel values for inflation radius & [string]\\

	\end{tabular}
}
\label{dynlayerparams}
\caption{dynamic\_cost\_layer parameters}
\end{table}




\section{MarkFreeSpace}

Like described in the concept, the purpose of this node is to provide data to the SLAM algorithm as well as to the costmaps. This data consists from the points on the polynomials of the road detection in combination with the filtered points of the lidar scan.\\
Based on the option of the obstacle\_layer of the costmap to raytrace marked obstacles, it needs to receive a PointCloud2 that contains points from the polynomials of the road detection. Raytracing the data removes the artifacts of previous measurements, as pictured in \ref{raytracing}. 
This displays two measurements, that are slightly rotated based on noise. Like pictured, without raytracing this would lead to a restricted road width in the costmap.

When looking at the ray-traced version, it is noticeable, that the old left road marking has been removed, since the newest measurement is behind it. The right road marking has not been removed, since the old measurement is behind the new one.

This allows to handle the noise of the camera and the road detection, which otherwise could significantly hinder the navigation, especially in the very limited room on the road.

\begin{figure} 
	\centering
	\includegraphics[width=.9\textwidth]{Pictures/raytracing road}
	\caption{Raytracing of the road data}
	\label{raytracing}
\end{figure}

If this point cloud would contain the points of the lidar as well, the obstacle layer would remove the points of the road marking, if an obstacle is behind it, as pictured in figure \ref{raytracing}.\\

\begin{figure} 
	\centering
	\includegraphics[width=.9\textwidth]{Pictures/raytracing}
	\caption{Combined vs. separate point clouds during raytracing}
	\label{raytracing}
\end{figure}

For the SLAM algorithm both data sources need to be fused into one point cloud. Therefore, the points need to be transformed into the same frame and time, before being published.\\

The data for the developed dynamic\_cost\_layer has to contain more information. It is in the form of a point cloud and contains channel values for point individual inflation radius, min-cost and max-cost.\\ 

This point cloud contains points of three types, left lane, right lane and obstacle.
The individual parameters for these points are pictured in \ref{markparams}. Using these parameters the cost distribution generated by the dynamic\_cost\_layer can be modified.\\

\begin{figure} 
	\centering
	\includegraphics[width=.7\textwidth]{Pictures/markfreespace parameter erklaerung}
	\caption{Top view of road with parameter}
	\label{markparams}
\end{figure}

The red points in the sketch represent the points of the left road marking. The distance between these points can be configured, in order to reduce the computational load on the dynamic\_cost\_layer. The same is possible for the blue points on the right road marking. Both points have a configurable radius, that is relative to the coverage of the right lane.\\
In addition to that the points of the left road marking receive max-cost and min-cost values for the required cost distribution. The points of the right road marking will only be used for the removal of costs.\\

The green point represents one of the measurements of the lidar sensor. Around this, the dynamic\_cost\_layer is supposed to generate a cost free zone, to improve path finding around obstacles.

The numbers in figure \ref{markparams} correspond to those of the following parameter list, that shows the specific names of all configurable parameters:

\begin{table} 
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ l l l }
	
 \textbf{Parameter} & \textbf{Description} & \textbf{Range}\\
 maxCost & Maximum cost of the left lane inflation & 0-254\\ 
 minCost & Minimum cost of the left lane inflation & 0-254\\  
 leftInflation & How far the inflation of the left lane overlaps the right lane (3) & 0-1\\  
 rightRemoval & How much of the right lane will be cleared from cost (4) & 0-1\\  
 obstacleRemoval & Distance around obstacles on the road where no cost gets placed (5) & 0-2 [m]\\  
 inflPointDistance & min Distance between inflated points 0 for only last (6) & 0-5 [m]\\ 
 clearPointDistance & min Distance between points that clear cost 0 for only last (7) & 0-2 [m]\\   
\end{tabular}
}
\caption{MarkeFreeSpace parameters}
\label{markfreespaceparams}
\end{table}

\subsection{Configuration}

The parameters in table \ref{markfreespaceparams} have to be configured so the robot will have a tendency to the right lane but still is able to drive on the left lane, if necessary. This means, that the maxcost has to be significantly smaller than the lethal (254). The Minimum cost has to be smaller than the neutral cost of the global planner, which defaults to 50. To complicate the transition to the left lane, the inflation radius of it is set to 50\% of the right lane and the radius of the right cost removal  is set to 40\% to guarantee a free space on the right road side.\\
Around obstacles the cost removal is set to 1.5 meter so the obstacle avoidance will finish within the 2 meter guideline defined in the Carolo-Cup rules\cite{carolocup}.\\

The distance between the points on one road marking correlates to the amount of inflated points. With a smaller distance more points will be inflated along the polynomial resulting in more computational load. Therefore the distances for the left and right points have to be configured fairly large. In case of the left road marking only the last point of the polynomial will be inflated, to prevent the inflation from closing the cleared section on the right lane. The distance between the points on the right road marking is set to 0.5 meter, so the entire length of the polynomial is cleared on the right lane.


The distances between the inflated points and the cleared points has to be fairly large to limit the computational load. For the inflation only the last point of the polynomial will be inflated, since a uniform cost distribution along the road marking is not too important. The distance between the cleared cells is set to 0.5 meters, to guarantee a clear section on the right lane.

The final configuration is pictured in Appendix \ref{config}.



\section{SLAM}
There are numerous lidar based SLAM packages available for ROS, but with the defined restriction of being able to use both, the points extracted from the road detection, as well as the lidar scan, most of the lidar based SLAM algorithms wont work since they only accept one lidar scan.\\
This rules out the popular options for lidar based SLAM like gmapping and HectorSLAM and the well documented package Google Cartographer will be used based on its flexibility in regards to robot configurations.\\
This package comes with an advantage of being based on loop closure, which might be useful when driving rounds in a circuit stile environment. The robot will drive the same route over and over again and thus the map could get more and more reliable over time even though the data is similar for different parts of the circuit and therefore not idal for the use with a SLAM algorithm.\\

Google Cartographer accepts numerous different input types including both point clouds and lidar scans. Additionally Google Cartographer can use provided odometry, as well as IMU data to improve the quality of the map.\\



\subsection{Configuration}
The goal of this node is to produce a map that gets more reliable over time.\\

Unfortunately, the data available for Cartographer is very self similar, meaning a straight road will always look the same and therefore does not have sufficient features for proper loop closure. In contrast to the points from the road detection the lidar can actually supply such features and will therefore be a good improvement for the resulting map.\\

But it is not guaranteed that the lidar will even see anything. Thus the SLAM algorithm has to work with the points of the road detection only as well.\\

The basic configuration of Cartographer is purely based on the setup of the robot. In this case Cartographer is supposed to use the lidar scan and the points of the road detection at the same time. To reduce the amount of times one of the sensor doesn't see anything, these will need to be merged in the markfreespace node and Cartographer will receive one PointCloud2 only. Cartographer is always relying on a continuous data stream of all sensors. If one of the sensor sources does not provide data Cartographer stops to publish the tf transform until every sensor supplies data again. Therefore the localization of the robot in the map is outdated and the usage of the map is not feasible.\\



To improve the map further the odometry supplied by the robot\_localization package is used as an input, as well as the IMU.

Furthermore Cartographer will be set to 2d map building.


\subsection{Tuning}
With the basic configuration of Cartographer, it is not able to provide a reliable map and a tuning procedure has to be performed. Here the general recommendation of the tuning guide should be followed, which states to tune local SLAM first and disabling global slam while doing so \cite{cartographertuning}.
To tune the local SLAM the parameter of the tracjectory builder have to be adjusted.\\
The trajectory builder contains a scan-matcher, which will compare incoming sensor data and tries to align it with each other as good as possible. This behavior can be tuned by configuring the size of the linear and angular search windows and the weight for the rotation and translation of the incoming scans.\\
One more important setting is the size of the sub maps. These can be adjusted by the amount of scans they contain. Since the submaps will consist out of the scan matched obstacles it is important to set the size of the submaps not to high, if the incoming data will be very self similar. Otherwise the scanmatcher will combine too many scans while shifting them over each other since they look so similar. This results in both rotational and translational error.\\
As soon as the local SLAM produces a reliable result after multiple rounds of the robot the global SLAM can be activated and tuned.\\

The global SLAM has two options of combining the submaps the loop-closure, which will check, if the robot was at this spot already, and a scanmatcher, which will try to match the submaps to the current scan. For both of these the weights can be adjusted individually and like in the local SLAM the size of the window for translation and rotation can be adjusted. Reducing the window size to a minimum is important when dealing with self similar data, so the submaps will not be shifted on top of each other. The size should be chosen so the global SLAM can still correct errors of the individual submaps.\\
With these values and the submaps the global planner calculates constraints between the maps which will be valued between 0 and 1. These constraints can be blocked with a threshold value that will block constraints with a smaller value. Like this the computation time can be drastically reduced and only the important constraints will be processed. Furthermore the weighting of the pose of the odometry and the local SLAM can be adjusted, which can be useful with bad odometry.

This tuning will be used in every test, in which cartographer is used.

\section{road\_detection}

Since the road detection of this project has been given, this node doesn't require any selection, but the individual nodes that make up the roadDetection package need to be configured, to work with the specific camera setup.

The nodes of the road detection are:
\begin{itemize}
	\item EdgeDetector
	\item EdgeProjector
	\item LineFinder
	\item LineTracker
	\item RoadDetector
	\item RoadRecordEvaluation
\end{itemize}

The EdgeDetector is the first node of the package and extracts the edges from the picture using canny filters. Here the expected line width in pixels has to be configured. The easiest way to determine the width is to run the node and take a look at the picture published on the topic ``/roadDetection/image\_edges''. Then rqt\_reconfigure can be used to raise the maxLineWidth parameter, until the entire line of the road is visible in the filtered picture. In case of the simulation the adjustment of the canny thresholds is not required, since the environment has only black and white colors and no greyscale.\\

The edgeProjector has the task to project the image on the 2D surface and therefore must know the position and orientation of the camera relative to the ground. While doing so it rectifies the image according to the provided camera information, which should contain the calibration parameters of the setup.

To get a reliable result from the ``LineFinder'' the amount of tiles in x and y has been increased to get more precise lines from the internally used hough transformation.\\
In this scenario the robot drives on a wider road than usual for the Carolo-Cup, so the parameter maxSegmentDistance has been increased to 0.9 m so the line segments of the middle line will still be matched to the same polynomial.\\
When the robot drives around a tight corner the road often starts at an angle relative to the robots x axis. The parameter ``maxStartAngleDiff'' has been increased to 75° to get more coverage in this case.\\

While configuring the road detection the first step is to configure the orientation and position of the camera in the node``EdgeProjector'' according to the definition in the URDF file. In addition to that the camera picture has to be cropped so the robot is not visible anymore in the picture.\\

In the ``LineTracker'' node the distance, at which lines should be evaluated needs to be set, so the node does not see the edge of the picture, in which the distortion has the largest effect.\\


The RoadDetector node needs an estimate for the lane width as a min and max value and a maximum angle between two individual road markings. These values are obviously specific for the environment.\\



